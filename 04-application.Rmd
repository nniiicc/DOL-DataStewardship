# Data Stewarship Applications

## Databases and SQL 


## Artificial Intelligence
Artificial Intelligence or AI is a field of computer science that uses formal logic to develop models that mimic human reasoning (hence, the term ‘artificial’ intelligence). AI is almost always aimed at tasks related to prediction, classification, or regression (i.e. estimating the relationships between a dependent variable and one or more independent variables). AI’s most basic goal in mimicking human reasoning is to automate tasks by turning data (examples) into models (recipes). In this sense, artificial intelligence models are algorithms - that is, models are algorithmic recipes that automate some task by using data to make a prediction, regression, or classification. 

In the past AI has struggled to make much progress on task automation for two reasons:
- insufficient data to train a model, and 
- insufficient computational power to run a model that can realistically mimic human reasoning.

Increasingly AI researchers have access to large amounts of well-structured data that can be used to train a model to classify or predict certain outcomes (e.g. The web contains lots of images of cats and dogs - and this data can be used to build a classifier to determine whether an image contains a dog or a cat). And, as the cost of high performance computing has come down there is also the opportunity to run models over very large datasets that can accurately make classifications or predictions. 

AI is increasingly described as anything that includes data and a prediction or decision-making algorithm. While there is great potential for AI applications in automating repetitive tasks there is a long way to go before these applications can realistically mimic human decision making or complex decision making. For example, if a person is walking a bike along the road an AI powering a driverless vehicle has a hard time determining whether or not this is a bicycle or a person - and this confusion can delay the decision making process that tells the car that it should swerve in order to miss the object (whether or not it is a person or a vehicle). This example resulted in the [first known death at the hands of AI](https://www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html)  

## Machine Learning 
Machine learning is a field of AI that contains all of the elements described above: data, tasks, and models. What machine learning attempts to do is optimize the automation of a task based on input data that is purposefully curated. To understand this process its helpful to first understand the broad difference between supervised vs unsupervised machine learning. In the broadest sense, these two forms of machine learning place emphasis on different aspects of an AI - supervised learning is teaching a machine to make decisions by example ( the emphasis is on data) and unsupervised learning is teaching a machine to make decisions by developing a sophisticated pattern making model (the emphasis on an algorithm).

- **Supervised machine learning** uses pre-labeled data that humans purposefully collect and categorize in order to train a model to perform a specific task. In some sense we can think of supervised learning as the flash card model of learning - By showing a machine thousands of examples of an image that is labeled as either a cat or a dog the machine can learn intricate features that distinguish cats from dogs. With enough examples (data) the machine can then look at a new image and make an accurate classification, that is whether the features of the image resemble what the machine “knows” about a cat or a dog. 

- **Unsupervised machine learning** also uses data, but instead of having thousands of diverse examples of labeled data a researcher will optimize a machine learning algorithm to learn patterns. Drawing on the example above about cats and dogs, an unsupervised machine learning application could be shown 100 example images of cats, dogs, monkeys, horses, and elephants. The machine would then learn features that distinguish these animals from one another, but in a shallow way. For example, if an image of a cat lacks pointy ears then the unsupervised machine learning application will likely mislabel the image as a dog. But, what the unsupervised machine learning approach lacks in accuracy it makes up for in diversity. Unsupervised machine learning can build complex patterns between data. This makes the unsupervised approach to machine learning very useful when labeled data isn’t easy or economical to use. 


**Further Reading**

- [IBM Primer on Machine Learning](https://www.ibm.com/cloud/learn/machine-learning) 
- [Classification, Regression, and Prediction](https://towardsdatascience.com/classification-regression-and-prediction-whats-the-difference-5423d9efe4ec) 
- [AI in the public sector](https://www.mckinsey.com/industries/public-and-social-sector/our-insights/when-governments-turn-to-ai-algorithms-trade-offs-and-trust) 


## Data Visualization 
Just as good writing depends on colorful examples, tone, correct grammar, and a logical progression of ideas so too does data analysis. Often data analysis results in not just a neat tidy number or result, but a set of results that need to be both explained in text and visually communicated through graphs, charts, or plots. The value of these data visualizations is not just to show off the results of an analysis through pretty pictures, but to economically communicate about data using a set of principles. This brief introduction to principles of data visualization is meant to guide future work, but it is by no means comprehensive. At the end of this section I provide a number of resources for further consultation. 

**Principles of data visualization** 

1. **Select Variables** - Data that are easy to visualize often contain a variable, an observation, and value. Each of these components of our data inform what is the best method of data visualization. Most frequently we want to depict a relationship between two or more variables in a set of data by showing how values change between observations. For example, if we want to communicate with a policymaker about the rate of traffic fatalities over time then we would identify a variable like traffic accidents in young drivers and select some observations that occur during a bounded period of time (e.g. 2005-2016). By showing how the values for these observations change we can demonstrate that there is a rise in traffic accidents, a decline, or maybe there is no steady trend at all. The point is that we identify in our data what relationship we want to depict, and then determine what variables can be used to demonstrate a relationship. Most often an independent variable (that is a variable that will occur regardless of any external event) is placed on the X axis and a dependent variable is placed on the Y axis. In the example above the independent variable is time and the dependent variable is traffic accidents. The relationship would then be depicted as follows: 

![Fatalities](https://wtsc.wa.gov/wp-content/uploads/2018/04/Young_Driver_Fatalities_20-1.png)
 
2. **Determine Scale of Values** - Data can be differentiated by many types and roles (as we’ve discussed throughout the curriculum). For data visualization often the first decision to make is whether the values of an observation are discrete or continuous. 
- Discrete values are whole or complete numbers that represent some real world phenomenon without an intermediary. In the example above, fatal crashes (the dependent variable) is a discrete value.There can’t be a partial fatality (either a driver died as the result of an accident or they did not). 
- Continuous values are numbers that can be partial, or divisible by some intermediary. In the example above the X axis depicts time by year. But, time is a continuous value - we could choose to represent time by hours, days, months, or years. 

Data may also contain categories or classifications that are neither discrete nor continuous. For example, if we were visualizing data related to traffic accidents we might have categories such as: 

- Head-on Collisions
- Highway Construction Accidents
- Intersection Accidents
- Interstate Accidents
- Rear-End Accidents
- Side-Impact Accidents

These are distinctions in the type of accident - they don’t have discrete or continuous values that can be easily visualized. Thus, it may be useful to use the categorical values together with discrete and continuous variable data values (e.g. image below) 

![Collisions](/Users/nmweber/Documents/GitHub/DOL-DataStewardship/images/v1.png)

3. **Determine Type of Variable Relationship** - If data visualization, at its core, is about clearly communicating relationships between variables in our data then it is worth knowing the types of common relationships that might occur, and what types of visualizations are best for communicating with each type. The three most common types of variable relationships that will be visualized are:

- Amount - 
- Distribution - 
- Proportion - 

There are a number of other relationship types that fall into and between these visualizations, including:  

- Correlation - how one variable relates to another (such as the number of times an accident occurs because of distraction) 
- Ranking 
- Time Series
- Deviation
- Distribution 
- Proportion / Part-Whole Relationship 

4. **Map elements to Aesthetics** - Finally, the graphical elements we choose to construct a visualization can feel overwhelming, but often they break down into three categories of visual aesthetics: 

- Position - All data visualizations are rendered in some form of spatial analysis - data for example can be visualized in two dimension position (with an X and Y axis), or data could have a more complex coordinate system that makes use of multiple dimensions (such as latitude, longitude, and depth on a map). Regardless of what dimensions are being depicted all visualizations must have a fixed position. 
- Shape Size and Color - all visualizations make use of shape, size, and color to communicate differences between variables, observations, and values. Data points can take different shapes (circles, triangles or squares) that communicate a different variable, or size (small, medium, or large) that communicate some effect size. Most often though, data visualization makes use of color (such as blue vs red) to depict differences in variables or observations that are being depicted.These three simple elements (shape, size, and color) are the essential building blocks of all visual grammars that allow for communicating differences between a variable, an observation, or a value being depicted. 
- Lines (or trends) - Visualizations often make use of lines to demonstrate a trend or a pattern (in our example above a downward trend was communicated with a sloping solid black line). Lines can be emphasized by their thickness, and lines can be differentiated by their type (e.g. solid, dashed, pointed lines). Often it is helpful to reduce the number of trend or shape lines in order to effectively communicate. 

**Resources for Data Visualization**

- [Fundamentals of Data Visualization](https://clauswilke.com/dataviz/) 
- [Data Visualization with R and GGPlot2](https://datavizs21.classes.andrewheiss.com/)
- [Contrast, Repetition, Alignment, and Proximity (CRAP visualizations)](https://www.presentationzen.com/chapter6_spread.pdf) 
